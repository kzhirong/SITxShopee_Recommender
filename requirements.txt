# Core ML Framework
torch>=2.0.0,<3.0.0
numpy>=1.24.0
scikit-learn>=1.3.0

# Data Processing
pandas>=2.0.0
PyYAML>=6.0
h5py>=3.8.0
pyarrow>=12.0.0
polars>=0.18.0
keras_preprocessing>=1.1.2  # Required by fuxictr tokenizer

# LLM Integration
transformers>=4.30.0,<5.0.0
# flash-attn: REQUIRED for all LLM training scripts
# Requires CUDA 11.8+ and Ampere+ GPU (A100, A6000, RTX 3090+, RTX 4090, etc.)
# Installation: pip install flash-attn --no-build-isolation
flash-attn>=2.0.0

# Utilities
tqdm>=4.65.0
matplotlib>=3.7.0

# Notes:
# - All LLM training scripts use flash_attention_2 (not optional)
# - For CPU-only testing, you'll need to modify training scripts to remove flash attention
# - Tested with Python 3.10+, PyTorch 2.0+, CUDA 11.8+
